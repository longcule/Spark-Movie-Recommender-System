{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>\n","    \n","<p>\n","<center>\n","<font size=\"5\">\n","Create API for Movie Recommend\n","</font>\n","</center>\n","<center>\n","<font size=\"4\">\n","Tạo API cho trang web đề xuất phim\n","</font>\n","</center>\n","</p>\n","    \n","</b>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p><h2>Table of Contents<span class=\"tocSkip\"></span></h2></p>\n","\n","<div class=\"toc\"><ul class=\"toc-item\">\n","    <li><span><a ><span>1&nbsp;&nbsp;</span>Overview</a></span></li>\n","    <li><span><a ><span>2&nbsp;&nbsp;</span>Import Library</a></span></li>\n","    <li><span><a ><span>3&nbsp;&nbsp;</span>Load Data</a></span></li>\n","    <li><span><a ><span>4&nbsp;&nbsp;</span>Tạo API cho trang web</a></span></li>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Overview "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Aim \n","- Tạo API cho trang web"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import Library"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import findspark\n","findspark.init()\n","from pyspark.sql.functions import monotonically_increasing_id \n","import os\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/05/27 14:40:52 WARN Utils: Your hostname, longcule-Inspiron-7591 resolves to a loopback address: 127.0.1.1; using 192.168.1.40 instead (on interface enx68e43b306c5a)\n","23/05/27 14:40:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/05/27 14:40:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","23/05/27 14:40:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n","23/05/27 14:40:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"]}],"source":["import findspark\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark import SparkContext, SparkConf\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Project\") \\\n","    .master(\"local[*]\") \\\n","    .config(\"spark.driver.memory\", \"16g\") \\\n","    .config(\"spark.executor.memory\", \"8g\") \\\n","    .config(\"spark.executor.cores\", \"8\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load Data"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+--------------------+--------------------+\n","|movieId|               title|              genres|\n","+-------+--------------------+--------------------+\n","|      1|    Toy Story (1995)|Adventure|Animati...|\n","|      2|      Jumanji (1995)|Adventure|Childre...|\n","|      3|Grumpier Old Men ...|      Comedy|Romance|\n","|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n","|      5|Father of the Bri...|              Comedy|\n","|      6|         Heat (1995)|Action|Crime|Thri...|\n","|      7|      Sabrina (1995)|      Comedy|Romance|\n","|      8| Tom and Huck (1995)|  Adventure|Children|\n","|      9| Sudden Death (1995)|              Action|\n","|     10|    GoldenEye (1995)|Action|Adventure|...|\n","|     11|American Presiden...|Comedy|Drama|Romance|\n","|     12|Dracula: Dead and...|       Comedy|Horror|\n","|     13|        Balto (1995)|Adventure|Animati...|\n","|     14|        Nixon (1995)|               Drama|\n","|     15|Cutthroat Island ...|Action|Adventure|...|\n","|     16|       Casino (1995)|         Crime|Drama|\n","|     17|Sense and Sensibi...|       Drama|Romance|\n","|     18|   Four Rooms (1995)|              Comedy|\n","|     19|Ace Ventura: When...|              Comedy|\n","|     20|  Money Train (1995)|Action|Comedy|Cri...|\n","+-------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["\n","mov = spark.read.csv('hdfs://127.0.0.1:9900/user/movies.csv', header=True,inferSchema=\"true\")\n","mov.show()\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","tags = spark.read.csv('hdfs://127.0.0.1:9900/user/tags_new.csv',header=True).drop('userId')\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'movieId,title,genres'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","read_movies_1m = sc.textFile(\"hdfs://127.0.0.1:9900/user/movies.csv\")\n","read_movies_1m_header = read_movies_1m.take(1)[0]\n","read_movies_1m_header"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","s_df = spark.read.format(\"csv\").option(\"header\", \"true\").load('hdfs://127.0.0.1:9900/user/movies.csv')\n","a = [i.movieId for i in s_df.select('movieId').distinct().collect()]\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/05/27 14:41:03 WARN BlockReaderFactory: I/O error constructing remote block reader.\n","java.nio.channels.ClosedByInterruptException\n","\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n","\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n","\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n","\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n","\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3025)\n","\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:826)\n","\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:751)\n","\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n","\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n","\tat java.io.DataInputStream.read(DataInputStream.java:149)\n","\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n","\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n","\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n","\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n","\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n","\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n","\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n","\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n","\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n","\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n","\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n"]},{"data":{"text/plain":["'movieId,tag'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","read_tags_1m = sc.textFile(\"hdfs://127.0.0.1:9900/user/tags_new.csv\")\n","read_tags_1m_header = read_tags_1m.take(1)[0]\n","read_tags_1m_header"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#creation of document for each movie in movies tags\n","read_tags_1m_data = read_tags_1m.filter(lambda line : line != read_tags_1m_header).map(lambda line:line.split(\",\")).map(lambda tokens:(int(tokens[0]),tokens[1]))\\\n",".groupByKey().sortByKey()\n","\n","documents = read_tags_1m_data.map(lambda x :   [i  for i in x[1]])\n","\n","read_tags = spark.createDataFrame(read_tags_1m_data,('movieid_num', 'document'))\n","\n","a = [i.movieid_num for i in read_tags.select('movieid_num').distinct().collect()]\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["57716"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\n","tag_movies = read_tags.join(mov, read_tags.movieid_num == mov.movieId).drop('movieId')\n","tag_movies_id = tag_movies.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n","tag_movies.count()\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["233"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["a.sort()\n","a.index(109487)\n","d1 = documents.collect()\n","d1[826]\n","a.index(236)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["DataFrame[words: string, frequency: bigint]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["105871\n"]}],"source":["\n","bagfwords = read_tags_1m.filter(lambda line : line != read_tags_1m_header).map(lambda line:line.split(\",\")).map(lambda tokens:((tokens[1]), int(1)))\\\n",".reduceByKey(lambda x,y:x+y)\n","dfbg = spark.createDataFrame(bagfwords,('words','frequency'))\n","display(dfbg)\n","features_length = bagfwords.count()\n","print(bagfwords.count())"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["57716"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#tags count\n","read_tags_1m_data.count()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from pyspark.mllib.feature import HashingTF, IDF\n","hashingTF = HashingTF(features_length)\n","tf = hashingTF.transform(documents)\n","tf.cache()\n","idf = IDF().fit(tf)\n","tfidf = idf.transform(tf)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def get_movie_docbyid(movieid):\n","  return read_tags_1m_data.filter(lambda x: x[0] == movieid).map(lambda x :  [i  for i in x[1]])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Make Recommend From movies"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import random\n","\n","def random_movie_genre(genres, df):\n","    matched_movies = df[df['genres'].str.contains(genres, case=False)]\n","\n","    if matched_movies.empty:\n","        return None\n","\n","    movie_ids = matched_movies['movieId'].tolist()\n","    random_movie_id = random.choice(movie_ids)\n","\n","    return random_movie_id\n","\n","# Đọc file CSV\n","df = pd.read_csv('data.csv')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from pyspark.mllib.feature import Normalizer\n","from pyspark.mllib.linalg import SparseVector, DenseVector\n","def get_movie_ids_gen(movie_id):\n","    \n","    new_movie = get_movie_docbyid(movie_id)\n","    candidateTf = hashingTF.transform(new_movie)\n","    candidateTfIdf = idf.transform(candidateTf)\n","    \n","    frequencyDenseVectors_0 = tfidf.map(lambda vector: DenseVector(vector.toArray()))\n","    frequencyDenseVectors_1 =  candidateTfIdf.map(lambda vector: DenseVector(vector.toArray()))\n","\n","    y1 =   frequencyDenseVectors_1.collect()\n","    re = frequencyDenseVectors_0.map(lambda x : (x.dot(y1[0]))/(x.norm(2)*y1[0].norm(2)))\n","\n","    result1=re.collect()\n","\n","\n","    dict1 = {}\n","    list_index  = []\n","    for i in result1:\n","      if (i > 0.1)&(i<0.999999) :\n","        dict1[a[result1.index(i)]] = i\n","    sorted_d = sorted(dict1.items(), key=lambda x: x[1])\n","    top5_r  = sorted_d[-20:]\n","    for i in top5_r:\n","      list_index.append(i[0])\n","\n","    return list_index"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Đọc file CSV\n","movie_gen = pd.read_csv('data.csv')\n","def random_movie_by_genre(genres):\n","    \n","    movieid = random_movie_genre(genres, movie_gen)\n","    result = get_movie_ids_gen(movieid)\n","    return result\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["movies = spark.read.csv(\"hdfs://127.0.0.1:9900/user/links.csv\", header=True, inferSchema=False)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","def show_imdbId_gen(genres):\n","    rec_df = spark.createDataFrame([(i,) for i in random_movie_by_genre(genres)], ['movieId'])\n","    result = rec_df.join(movies,['movieId'])\n","    imdbid = result.select(result['imdbId'].cast('string')).rdd.flatMap(lambda x: x).collect()\n","    imdbid_with_tt = ['tt' + imdbid for imdbid in imdbid]\n","    return imdbid_with_tt\n","# genres_1 = 'Drama'\n","# show_imdbId_gen(genres_1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Recom movieId"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from pyspark.mllib.feature import Normalizer\n","from pyspark.mllib.linalg import SparseVector, DenseVector\n","def get_movie_ids(movie_id):\n","    \n","    new_movie = get_movie_docbyid(movie_id)\n","    candidateTf = hashingTF.transform(new_movie)\n","    candidateTfIdf = idf.transform(candidateTf)\n","    \n","    frequencyDenseVectors_0 = tfidf.map(lambda vector: DenseVector(vector.toArray()))\n","    frequencyDenseVectors_1 =  candidateTfIdf.map(lambda vector: DenseVector(vector.toArray()))\n","\n","    y1 =   frequencyDenseVectors_1.collect()\n","    re = frequencyDenseVectors_0.map(lambda x : (x.dot(y1[0]))/(x.norm(2)*y1[0].norm(2)))\n","\n","    result1=re.collect()\n","\n","\n","    dict1 = {}\n","    list_index  = []\n","    for i in result1:\n","      if (i > 0.1)&(i<0.999999) :\n","        dict1[a[result1.index(i)]] = i\n","    sorted_d = sorted(dict1.items(), key=lambda x: x[1])\n","    top5_r  = sorted_d[-20:]\n","    for i in top5_r:\n","      list_index.append(i[0])\n","\n","    return list_index"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["\n","def show_imdbId_movieid(movie_id):\n","    rec_df = spark.createDataFrame([(i,) for i in get_movie_ids(movie_id)], ['movieId'])\n","    result = rec_df.join(movies,['movieId'])\n","    imdbid = result.select(result['imdbId'].cast('string')).rdd.flatMap(lambda x: x).collect()\n","    imdbid_with_tt = ['tt' + imdbid for imdbid in imdbid]\n","    return imdbid_with_tt\n","# movie_id = 1\n","# show_imdbId_movieid(movie_id)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Recom UserId"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["recommendation = spark.read.parquet('/home/longcule/Documents/bigdata/recommendation.parquet')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+--------------------+\n","|userId|     recommendations|\n","+------+--------------------+\n","|     1|[{183947, 4.70272...|\n","|     3|[{184299, 5.03557...|\n","|     5|[{140224, 4.85786...|\n","|     6|[{156414, 4.93688...|\n","|     9|[{2858, 4.962009}...|\n","+------+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["recommendation.show(5)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import explode\n","from pyspark.sql.functions import mean, col\n","def get_recommendations(user_id):\n","    recs = recommendation.filter(col(\"userId\") == user_id).select(\"recommendations\")\n","    recs = recs.select(explode(col(\"recommendations\")).alias(\"rec\")).select(\"rec.movieId\", \"rec.rating\")\n","    item_list = recs.orderBy(col(\"rating\").desc()).select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n","    return item_list"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["links = spark.read.csv(\"hdfs://127.0.0.1:9900/user/links.csv\", header=True, inferSchema=False)\n","\n","def show_imdbId(user_id):\n","    rec_df = spark.createDataFrame([(i,) for i in get_recommendations(user_id)], ['movieId'])\n","    result = rec_df.join(links,['movieId'])\n","    imdbid = result.select(result['imdbId'].cast('string')).rdd.flatMap(lambda x: x).collect()\n","    imdbid_with_tt = ['tt' + imdbid for imdbid in imdbid]\n","    return imdbid_with_tt\n","\n","# show_imdbId(10)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### FLASK create API "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from flask import Flask, jsonify\n","\n","app = Flask(__name__)\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["@app.after_request\n","def add_cors_headers(response):\n","    response.headers['ngrok-skip-browser-warning'] = '12382' # Cho phép từ tất cả các nguồn truy cập\n","    return response"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["@app.route('/recommendations/<int:user_id>', methods=['GET'])\n","def get_recommendations_api(user_id):\n","    item_list = show_imdbId(user_id)\n","    return jsonify({'recommendations': item_list})"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["@app.route('/recommendations/movieid/<int:movie_id>', methods=['GET'])\n","def get_imdbid(movie_id):\n","    imdb_ids = show_imdbId_movieid(movie_id)\n","    return jsonify({'movieId': imdb_ids})\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["@app.route('/recommendations/genres/<string:genres>', methods=['GET'])\n","def get_imdbid_by_genre(genres):\n","    imdb_ids = show_imdbId_gen(genres)\n","    return jsonify({'genres': imdb_ids})"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["127.0.0.1 - - [27/May/2023 14:42:20] \"GET /recommendations/genres/Drama HTTP/1.1\" 200 -\n","127.0.0.1 - - [27/May/2023 14:43:04] \"GET /recommendations/movieid/1 HTTP/1.1\" 200 -\n"]}],"source":["if __name__==\"__main__\":\n","    app.run(host=os.getenv('IP', '127.0.0.1'), \n","            port=int(os.getenv('PORT', 5000\n","            )))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"name":"content-based","notebookId":3958629019109705},"nbformat":4,"nbformat_minor":0}
